# ğŸ” ClipCap Vision: Semantic Image & Video Search + Captioning

[![MIT License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.8%2B-blue.svg)](https://www.python.org/)
[![Streamlit](https://img.shields.io/badge/streamlit-ready-brightgreen)](https://streamlit.io/)

> **AI-powered, real-time search, captioning, and analytics for surveillance, media, and enterprise.**
>  
> **Find, caption, and analyze both images and videos using CLIP+LoRA, BLIP, and a modern RAG Q&A pipeline.**

---

## ğŸš€ Features

- ğŸ” **Multi-Modal Semantic Search**  
  - Natural language queries for both images and videos  
  - Example:  
    - `no helmet in last 10 minutes from camera_1`
    - `person with a dog`
    - `show me video of a man in a red shirt`

- ğŸ“¸ **Real-Time Ingestion & Indexing**  
  - Auto-monitors folders for new images and videos  
  - Extracts frames from videos, tags with CLIP/LoRA + BLIP

- ğŸ¤– **RAG Q&A (Retrieval-Augmented Generation)**  
  - Ask questions about your data (images or videos)  
  - Centralized, context-aware answers (not just search)

- ğŸ“Š **Live Analytics Dashboard**  
  - Hourly trends, camera-wise distribution, confidence histograms  
  - Video analytics: violations per video, duration, event timeline

- âš™ï¸ **Modern Streamlit UI**  
  - Tabs for image and video search  
  - Upload, search, caption, and analytics in one place

- ğŸ—ï¸ **Production-Ready Pipeline**  
  - ChromaDB for fast, scalable vector search  
  - SQLite for metadata/analytics  
  - Docker support for easy deployment

---

## ğŸ§  How It Works

[Images/Videos] â†’ [Observer] â†’ [Frame Extraction] â†’ [CLIP/LoRA + BLIP Tagging] â†“ [ChromaDB (Embeddings + Captions)] â† [RAG Q&A] â† [User Query] â†“ [Streamlit UI: Search, Q&A, Analytics] â†“ [SQLite: Metadata, Analytics]


- **Images and videos are ingested in real time.**
- **Videos are split into frames, each frame is tagged and indexed.**
- **All embeddings and captions are stored in ChromaDB for fast semantic search.**
- **Metadata is stored in SQLite for analytics and reporting.**
- **RAG Q&A lets you ask questions about your data and get context-aware answers.**
- **Streamlit UI provides a modern, user-friendly interface for search, upload, and analytics.**

---

## ğŸ‹ï¸â€â™‚ï¸ Model Training Pipeline

- **CLIP + LoRA**:  
  - Pretrained on Flickr30k for general vision-language grounding  
  - Fine-tuned on your custom surveillance/violation dataset for domain adaptation

- **BLIP**:  
  - Used for automatic, high-quality captioning of both images and video frames

- **RAG Q&A**:  
  - Centralized, multi-modal search and Q&A over both images and videos

---

## ğŸ–¼ï¸ Screenshots

### Image Search
![Image Search UI](./doc/image_search_ui.png)

### Video Search
![Video Search UI](./doc/video_search_ui.png)

### Analytics Dashboard
![Analytics Dashboard](./doc/analytics_dashboard.png)

### Top Violation Types
![Top Violation Types](./doc/violation_types.png)

### Video Event Timeline
![Video Event Timeline](./doc/video_event_timeline.png)

---

## ğŸ§­ Project Structure

```text
semantic-image-search/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main_app.py           # Streamlit UI (image/video search, Q&A)
â”‚   â”œâ”€â”€ observer.py           # Real-time file/video watcher
â”‚   â”œâ”€â”€ video_chunker.py      # Frame extraction + video metadata
â”‚   â”œâ”€â”€ frame_tagger.py       # Frame tagging (CLIP/LoRA + BLIP)
â”‚   â”œâ”€â”€ database.py           # SQLite metadata manager
â”‚   â”œâ”€â”€ analytics_dashboard.py # Analytics dashboard
â”‚   â””â”€â”€ utils.py              # Embedding, captioning, helpers
â”‚
â”œâ”€â”€ scripts/                  # CLI tools for dev/test
â”‚   â”œâ”€â”€ dummy_image_violation_generator.py
â”‚   â”œâ”€â”€ dummy_video_violation_generator.py
â”‚   â””â”€â”€ video_splitter.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ cache/
â”‚   â””â”€â”€ metadata.db
â”œâ”€â”€ images/                   # Indexed images/videos
â”œâ”€â”€ frames/                   # Extracted video frames
â”œâ”€â”€ video_chunks/             # Video chunks for testing
â”œâ”€â”€ doc/                      # README assets, screenshots, etc.
â””â”€â”€ requirements.txt
ğŸ› ï¸ Getting Started
# 1. Clone the repository
git clone https://github.com/debjit721212/semantic-image-search.git
cd semantic-image-search

# 2. (Optional) Setup virtual environment
python3 -m venv env
source env/bin/activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Launch the Streamlit app
streamlit run app/main_app.py

ğŸ³ Docker Support
# Build image
docker build -t clipcap-vision .

# Run the app
docker run -p 8501:8501 -v $(pwd)/images:/app/images clipcap-vision

# Or use docker-compose (if needed)
docker-compose up --build

# Stop it
docker-compose down

ğŸ“Š Live Analytics Dashboard
Filter by camera ID or hours
Hourly violation bar chart
Camera usage pie chart
Confidence histogram
Video analytics: violations per video, duration, event timeline
Full metadata table
ğŸ§ª Future Enhancements
âœ… REST API for remote querying
âœ… Docker + deployment automation
ğŸ”² Multi-modal alert system
ğŸ”² Replace SQLite with Qdrant or Weaviate
ğŸ”² Live camera stream captioning
ğŸ”² Video event summarization with LLMs
ğŸ™Œ Acknowledgements
OpenAI CLIP
Salesforce BLIP
Streamlit
ChromaDB
SQLite
Docker
ğŸ“œ License
MIT License â€” See LICENSE

Built with â¤ï¸ by @debjit721212

ğŸ’¡ Why This Project?
Solves real-world surveillance and media search problems
Handles both images and videos, with RAG Q&A
Production-ready, scalable, and open source
Modern, modular, and easy to extend

Try it, contribute, and make AI-powered vision search better for everyone!